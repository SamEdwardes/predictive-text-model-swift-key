---
title: "Week 2 - Milestone Report"
author: Sam Edwardes
date: 2019-05-08
output: html_notebook
---

```{r setup, warning=FALSE, message=FALSE, echo=FALSE}
# kintr settings
if(Sys.info()["sysname"] == "Windows"){ # working directory for my windows machine
    knitr::opts_knit$set(root.dir = "~/GitHub/predictive-text-model-swift-key")
} else { # working directory for my Macbook Pro
    knitr::opts_knit$set(root.dir = "~/Documents/GitHub/predictive-text-model-swift-key")
}

# libraries
library(quanteda)
library(readtext)
library(tidyverse)
```


## Background

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to: 

1. Demonstrate that you've downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.

## Part 1 - Loading the data

The data was downloaded from [this link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) and unzipped to my local disc. From here the text documents were loaded into r using the libraries *readtext* and *quanteda*. For my analysis, only the english news, twitter, and blog files were used.

```{r}
news_big <- readtext("data/raw/en_US.news.txt")
blogs_big <- readtext("data/raw/en_US.blogs.txt")
twitter_big <- readtext("data/raw/en_US.twitter.txt")

# read all data
news_big <- readtext("data/raw/en_US.news.txt")
blogs_big <- readtext("data/raw/en_US.blogs.txt")
twitter_big <- readtext("data/raw/en_US.twitter.txt")

# create corpus
news_corp <- corpus(news_big)
blogs_corp <- corpus(blogs_big)
twitter_corp <- corpus(twitter_big)

# segment the corpus to lines
news_corp <- corpus_segment(news_corp, pattern = "\n")
blogs_corp <- corpus_segment(blogs_corp, pattern = "\n")
twitter_corp <- corpus_segment(twitter_corp, pattern = "\n")

# add the source column
docvars(news_corp, "source") <- "news"
docvars(blogs_corp, "source") <- "blog"
docvars(twitter_corp, "source") <- "twitter"

# add all corpus together
all_corp <- news_corp + blogs_corp + twitter_corp

# calcualte length and add to corpus
len <- sapply(texts(all_corp), nchar)
docvars(all_corp, "length") <- len


# remove large files from memory
# rm(news_corp, blogs_corp, twitter_corp, news_big, blogs_big, twitter_big)
```


```{r}
summary(all_corp)
```


## Part 2 - Summary statistics

Lets review some summaries of the data data we have loaded into R. 

There are exactly `r nrow(all_corp$documents)` text documents that have been loaded. The breakdown of these documents between the three sources of news, blog, and twitter is as follows: 

```{r}
table((all_corp$documents)[,"source"])
```

The general trend of text length (number of characters is as follows):

```{r warning=FALSE, message=FALSE}
ggplot(all_corp$documents, aes(x = length, fill = source)) + 
    geom_histogram() + 
    xlim(0, 1000) + 
    ggtitle("Text Length (Number of Characters)")
    
```

We are also interested to see what words appear with the most frequency. To accomplish this, we will tokenize the data, then create a data feature matrix (dfm) to see which words occur most frequently.

*A sample of the corpus was used in order to speed up the analysis.*

```{r}
# set seed
set.seed(1993)

# select a corpus sample
sample_corp <- corpus_sample(all_corp, size = 500000)

# tokenize the documents
sample_tokens <- tokens(sample_corp, remove_numbers = TRUE, remove_punct = TRUE)
sample_tokens <- tokens_tolower(sample_tokens)

# create a dfm
sample_dfm <- dfm(sample_tokens)

# summarise the top n words
topfeatures(sample_dfm, 100)
```

Part 3 - Interesting observations

One of the most frequently occurring words was **"â"**. In order to understand how this word should be treated, we should understand how it was used in context. Lets find an example of this word in action.

```{r}
# View 5 examples
head(kwic(sample_tokens, "â"),5)

# lets look at a specific example from blogs and twitter
texts(sample_corp)["en_US.twitter.txt.1239564"]
texts(sample_corp)["en_US.twitter.txt.2014655"]
```

As can be seen, this symbol does not appear to have a lot of meaning. It is likely to do with HTML formatting. When we build our model we will considering removing this symbol, or replacing with with something else.

Another frequently occurring token was **rt**. Let's look at this token in context as well.

```{r}
# View 5 examples
head(kwic(sample_tokens, "rt"),5)

# lets look at a specific example from blogs and twitter
texts(sample_corp)["en_US.twitter.txt.2014655"]
texts(sample_corp)["en_US.twitter.txt.2205456"]
```

Similar to the last example, "RT" doesn't seem to have a lot of meaning or value in predicting the next word. It likely refers to a "re-tweet" in twitter. When building our model we will consider removing the token RT.
---
output: html_notebook
---

## Background

Goal of this doc is to understand:

- Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
- Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

Questions to consider:

1. Some words are more frequent than others - what are the distributions of word frequencies?
2. What are the frequencies of 2-grams and 3-grams in the dataset?
3. How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
4. How do you evaluate how many of the words come from foreign languages?
5.Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?


## Load the data

```{r setup, warning=FALSE, message=FALSE}
# kintr settings
knitr::opts_knit$set(root.dir = "~/GitHub/predictive-text-model-swift-key")
```

```{r warning=FALSE, message=FALSE}
# libraries
library(quanteda)
library(readtext)
library(tidyverse)
```


```{r}
# read all data
news_big <- readtext("data/raw/en_US.news.txt")
blogs_big <- readtext("data/raw/en_US.blogs.txt")
twitter_big <- readtext("data/raw/en_US.twitter.txt")

# create corpus
news_corp <- corpus(news_big)
blogs_corp <- corpus(blogs_big)
twitter_corp <- corpus(twitter_big)

# segment the corpus to lines
news_corp <- corpus_segment(news_corp, pattern = "\n")
blogs_corp <- corpus_segment(blogs_corp, pattern = "\n")
twitter_corp <- corpus_segment(twitter_corp, pattern = "\n")

# add all corpus together
text_corp <- news_corp + blogs_corp + twitter_corp

# create a random sample that will be used for testin functions faster
set.seed(2019-07-24)
sample_corp <- corpus_sample(text_corp, size = 100000)
```


## Add some features to the corpus
```{r}
# calcualte length and add to corpus
len <- sapply(texts(sample_corp), nchar)
docvars(sample_corp, "Length") <- len
```

## Questions to answer

### Q #1

Some words are more frequent than others - what are the distributions of word frequencies? 

```{r}
# create the tokens
doc.tokens <- tokens(sample_corp)

# clean the tokens
doc.tokens <- tokens(doc.tokens, remove_numbers = TRUE, remove_punct = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords("english"), selection = "remove")
doc.tokens <- tokens_tolower(doc.tokens)

head(doc.tokens)
```

Distribution of word frequencies:
```{r}
# create a document feature matrix (dfm)
doc.dfm <- dfm(doc.tokens)

# find top n features
topfeatures(doc.dfm, 10)
```
 
 We see that â comes up a lot, what does this mean?
```{r}
# View 100 examples
View(kwic(doc.tokens, "â"),100)

# lets look at a specific example
texts(sample_corp)["en_US.blogs.txt.631388"]
```

It looks like â should mean "and". Lets replace it and then run the top features again.
```{r}
doc.tokens <- tokens_replace(doc.tokens, pattern = "â", replacement = "and")
doc.dfm <- dfm(doc.tokens)
topfeatures(doc.dfm, 10)
```

S is also showing up by itself a lot. Whats up with that?
```{r}
View(kwic(doc.tokens, "s"), 500)

# lets look at a specific example
texts(sample_corp)[c("en_US.blogs.txt.742590", "en_US.blogs.txt.310608")]
```

it looks like "s" shows up after an apostraphe, e.g. there's. It is probably just safe to delete for now
```{r}
doc.tokens <- tokens_select(doc.tokens, pattern = "s", selection = "remove")
doc.dfm <- dfm(doc.tokens)
topfeatures(doc.dfm, 60)
```

### Q #2

What are the frequencies of 2-grams and 3-grams in the dataset?
```{r}
# first, generate some ngrams from the tokens we created

toks.ngram.2 <- tokens_ngrams(doc.tokens, n = 2)
head(toks.ngram.2)

toks.ngram.3 <- tokens_ngrams(doc.tokens, n = 3)
head(toks.ngram.3)
```

Create a dfm for the ngrams
```{r}
ngrams.2.dfm <- dfm(toks.ngram.2)
topfeatures(ngrams.2.dfm, 20)
```

```{r}
ngrams.3.dfm <- dfm(toks.ngram.3)
topfeatures(ngrams.3.dfm, 20)
```

It looks like we still have some questionable characters in our tokens. Lets start fresh and try and get rid of them

- https://stackoverflow.com/questions/55435345/how-do-i-efficiently-replace-a-vector-of-strings-with-another-pairwise-in-a
- https://stackoverflow.com/questions/55435345/how-do-i-efficiently-replace-a-vector-of-strings-with-another-pairwise-in-a

```{r}
# start from scratch, get a sample corpus
set.seed(2019-07-24)
sample_corp <- corpus_sample(text_corp, size = 100000)

# replace errors in corpus to avoid problems when tokenizing
texts(sample_corp) <- gsub(pattern = "â€™", replacement = "'", x = texts(sample_corp))
texts(sample_corp) <- gsub(pattern = " â ", replacement = "and", x = texts(sample_corp))
texts(sample_corp) <- gsub(pattern = "â€œ", replacement = "", x = texts(sample_corp))

# remove anything that is not alpha or number
texts(sample_corp) <- stringr::str_replace_all(texts(sample_corp),"[^a-zA-Z\\s]", "")

# create the tokens
doc.tokens <- tokens(sample_corp)

# clean the tokens
doc.tokens <- tokens(doc.tokens, remove_numbers = TRUE, remove_punct = TRUE,
                     remove_separators = TRUE, remove_symbols = TRUE)
doc.tokens <- tokens_select(doc.tokens, stopwords("english"), selection = "remove")
doc.tokens <- tokens_tolower(doc.tokens)

# get rid of bad tokens
doc.tokens <- tokens_replace(doc.tokens, pattern = "â", replacement = "and")
doc.tokens <- tokens_select(doc.tokens, pattern = "s", selection = "remove")

# replace bad characters
patterns <- c("â")
doc.tokens <- as.tokens(lapply(doc.tokens, gsub, pattern = patterns, replacement = ""))

# word stem
# doc.tokens <- tokens_wordstem(doc.tokens)
```


```{r}
toks.ngram.2 <- tokens_ngrams(doc.tokens, n = 2)

ngrams.2.dfm <- dfm(toks.ngram.2)
topfeatures(ngrams.2.dfm, 20)
```



```{r}
toks.ngram.3 <- tokens_ngrams(doc.tokens, n = 3)
ngrams.3.dfm <- dfm(toks.ngram.3)
topfeatures(ngrams.3.dfm, 20)
```


